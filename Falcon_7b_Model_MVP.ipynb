{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Chekireb/falcon-llm-t4-gpu-chat-evaluation/blob/main/Falcon_7b_Model_MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHqp8NyXZnkq"
      },
      "source": [
        "### 1.Import Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M635AuVJDb5"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FPcXjaBZ8pE"
      },
      "outputs": [],
      "source": [
        "! pip install langchain\n",
        "# !pip install faiss-cpu\n",
        "! pip install transformers\n",
        "# !pip install auto_gptq\n",
        "# !pip install -qqq InstructorEmbedding==1.0.1\n",
        "! pip install pypdf\n",
        "! pip install sentence_transformers\n",
        "! pip install transformers\n",
        "# This library helps you run PyTorch code on any device.\n",
        "# It simplifies the code needed for using multiple GPUs, TPUs, or mixed precision\n",
        "! pip install accelerate\n",
        "# This is a library for manipulating tensors.\n",
        "# It provides a more readable and expressive way to handle tensors in deep learning and scientific computing\n",
        "! pip install einops\n",
        "! pip install xformers\n",
        "! pip install langchain\n",
        "! pip install langchain_community\n",
        "# !pip install -U langchain-community\n",
        "! pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF96i95R2RMw"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline # AutoModelForCausalLM is used to add layer for application like QA\n",
        "import langchain\n",
        "from langchain import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Creat the Vectore Data Base"
      ],
      "metadata": {
        "id": "OMzBTfBqzIk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEweeyeZZyOd"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "DATA_PATH = '/content/'\n",
        "DB_FAISS_PATH = 'vectorstore/db_chroma'\n",
        "\n",
        "# Create vector database\n",
        "def create_vector_db():\n",
        "    loader = DirectoryLoader(DATA_PATH,\n",
        "                             glob='*.pdf',\n",
        "                             loader_cls=PyPDFLoader)\n",
        "\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                                   chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    # embeddings = HuggingFaceInstructEmbeddings(\n",
        "    # model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
        "# )\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': DEVICE})\n",
        "\n",
        "    # 4. Creat Vector Data Base\n",
        "    db = Chroma.from_documents(texts,embeddings,persist_directory=DB_FAISS_PATH)\n",
        "    # store the db in repertory\n",
        "    db.persist()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.Initiate a model"
      ],
      "metadata": {
        "id": "zJszH0YYzPUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3. Falcon 7B"
      ],
      "metadata": {
        "id": "BZOlss7LWMTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"tiiuae/falcon-7b-instruct\" #tiiuae/falcon-40b-instruct\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    truncation=True,\n",
        "    max_new_tokens=256,\n",
        "    max_length=1024,\n",
        "    do_sample=True,\n",
        "    # top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    return_full_text=False\n",
        ")"
      ],
      "metadata": {
        "id": "xe8vKQbhYhCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs={\"truncation\": True, \"temperature\":0.1})"
      ],
      "metadata": {
        "id": "UwT4T2netpnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Can you tell me a fact about the moon ?\\n?\")"
      ],
      "metadata": {
        "id": "gvWTIKmWeVcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm(\"Can you tell me a fact about the moon ?\")"
      ],
      "metadata": {
        "id": "FxJDxC4euTEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttr1wtFNy72w"
      },
      "source": [
        "### 4.Output Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### 4.1. create_retrieval_chain"
      ],
      "metadata": {
        "id": "kruOQUcz3bEm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8TvpjIji6bt"
      },
      "source": [
        "######4.1.1.Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP1ly59qXYEA"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry6oSjO6i82v"
      },
      "source": [
        "######4.1.2. Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOBqi48BXYBY"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': DEVICE})\n",
        "VDBPath = 'vectorstore/db_chroma'\n",
        "vdb_connection = Chroma(persist_directory=VDBPath,  embedding_function=embeddings)\n",
        "# Create new connection to VDB\n",
        "retriever = vdb_connection.as_retriever(search_kwargs={'k': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2XJWmaGj3Wo"
      },
      "source": [
        "### Contextualize question to be understood without need to chat history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He6OjfYPXX_C"
      },
      "outputs": [],
      "source": [
        "# # Contextualize question from history\n",
        "# contextualize_q_system_prompt = (\n",
        "# \"\"\"Given this chat history and the latest user question which might reference context in the chat history,\\\n",
        "#  formulate a standalone question which can be understood\\\n",
        "#   without the chat history. Do NOT answer the question, just\\\n",
        "#   reformulate it if needed and otherwise return it as is.\"\"\")\n",
        "# contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", contextualize_q_system_prompt),\n",
        "#         MessagesPlaceholder(\"chat_history\"),\n",
        "#         (\"human\", \"{input}\"),\n",
        "#     ]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXcO79XiXX8g"
      },
      "outputs": [],
      "source": [
        "# # Function creates a retriever that uses chat history and a language model to\n",
        "# # retrieve relevant documents for answering user questions.\n",
        "# history_aware_retriever = create_history_aware_retriever(\n",
        "#     llm, retriever, contextualize_q_prompt\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVItMsI-lluB"
      },
      "source": [
        "######4.1.3. Answering question prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj9x4rJbXX6G"
      },
      "outputs": [],
      "source": [
        "# # Answer question\n",
        "# qa_system_prompt = (\n",
        "#     \"Use \"\n",
        "#     \"the  context and the following chat history to answer and assist the Human. \"\n",
        "#     \"If you don’t know the answer to a question, just say that you do not know. \"\n",
        "#     \"The response should be short, concise and exhaustive. \"\n",
        "#     \"\\n Context : {context}\"\n",
        "#     # \"\\n Chat History : {chat_history}\"\n",
        "# )\n",
        "\n",
        "# qa_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", qa_system_prompt),\n",
        "#         MessagesPlaceholder(\"chat_history\"),\n",
        "#         (\"human\", \"{input}\"),\n",
        "#     ]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JILgInMzXXzv"
      },
      "outputs": [],
      "source": [
        "# # Below we use create_stuff_documents_chain to feed all retrieved context\n",
        "# # into the LLM. Note that we can also use StuffDocumentsChain and other\n",
        "# # instances of BaseCombineDocumentsChain.\n",
        "# question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "# rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtpcBwPlftVs"
      },
      "outputs": [],
      "source": [
        "# #output function\n",
        "# def final_result(query, history):\n",
        "#   result = rag_chain.invoke({\"input\": query, \"chat_history\": history})\n",
        "#   history.append(('human', query))\n",
        "#   history.append(('assistant', result['answer']))\n",
        "\n",
        "#   def keep_last_n_tuples(lst, n=4):\n",
        "#     return lst[-n:] if len(lst) > n else lst\n",
        "\n",
        "#   history = keep_last_n_tuples(history)\n",
        "#   return result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBKucJ1IueZD"
      },
      "outputs": [],
      "source": [
        "# history=[]\n",
        "# while True:\n",
        "#     query = input(\"What's on your mind? (Type 'exit' to end): \")\n",
        "#     if query.lower() == \"exit\":\n",
        "#         # history.clear()\n",
        "#         break\n",
        "\n",
        "#     res = final_result(query,history)\n",
        "#     print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"I would like the contacts of a FRONTEND DEVELOPER\"\n",
        "# # history = []\n",
        "# result = rag_chain.invoke({\"input\": query, \"chat_history\": history})\n",
        "# history.append(('human', query))\n",
        "# history.append(('system', result['answer']))\n",
        "\n",
        "# def keep_last_n_tuples(lst, n=4):\n",
        "#   return lst[-n:] if len(lst) > n else lst\n",
        "\n",
        "# history = keep_last_n_tuples(history)\n",
        "# result['answer']"
      ],
      "metadata": {
        "id": "g2IUt3P19_y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(result['answer'])"
      ],
      "metadata": {
        "id": "DtXopwvYEck2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En6lOHZZ5i7D"
      },
      "source": [
        "### RAG From LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iocLV3nw5mf7"
      },
      "outputs": [],
      "source": [
        "# import bs4\n",
        "# from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# from langchain_chroma import Chroma\n",
        "# from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "# from langchain_community.document_loaders import WebBaseLoader\n",
        "# from langchain_core.chat_history import BaseChatMessageHistory\n",
        "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# ### Contextualize question ###\n",
        "# contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
        "# which might reference context in the chat history, formulate a standalone question \\\n",
        "# which can be understood without the chat history. Do NOT answer the question, \\\n",
        "# just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "\n",
        "# contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", contextualize_q_system_prompt),\n",
        "#         MessagesPlaceholder(\"chat_history\"),\n",
        "#         (\"human\", \"{input}\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# history_aware_retriever = create_history_aware_retriever(\n",
        "#     llm, retriever, contextualize_q_prompt\n",
        "# )\n",
        "\n",
        "\n",
        "# ### Answer question ###\n",
        "# qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
        "# Use the following pieces of retrieved context to answer the question. \\\n",
        "# If you don't know the answer, just say that you don't know. \\\n",
        "# Use three sentences maximum and keep the answer concise.\\\n",
        "\n",
        "# {context}\"\"\"\n",
        "# qa_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", qa_system_prompt),\n",
        "#         MessagesPlaceholder(\"chat_history\"),\n",
        "#         (\"human\", \"{input}\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "\n",
        "# rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "\n",
        "# ### Statefully manage chat history ###\n",
        "# store = {}\n",
        "\n",
        "\n",
        "# def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "#     if session_id not in store:\n",
        "#         store[session_id] = ChatMessageHistory()\n",
        "#     return store[session_id]\n",
        "\n",
        "\n",
        "# conversational_rag_chain = RunnableWithMessageHistory(\n",
        "#     rag_chain,\n",
        "#     get_session_history,\n",
        "#     input_messages_key=\"input\",\n",
        "#     history_messages_key=\"chat_history\",\n",
        "#     output_messages_key=\"answer\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.2.Simple Chain"
      ],
      "metadata": {
        "id": "uho8lXJ5hlPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n"
      ],
      "metadata": {
        "id": "gpW-5kzKhk23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new connection to VDB\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': DEVICE})\n",
        "VDBPath = 'vectorstore/db_chroma'\n",
        "vdb_connection = Chroma(persist_directory=VDBPath,  embedding_function=embeddings)\n"
      ],
      "metadata": {
        "id": "U73P6P-dh0ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output function\n",
        "def final_result(question):\n",
        "  docs = vdb_connection.similarity_search(question,k=2)\n",
        "  context = \"\\n\\n \".join([doc.page_content for doc in docs])\n",
        "\n",
        "  promp_template = \"Answer this Question ({question}) based only on the given context.\\n\\n\" \\\n",
        "+ f\"Context: {context}\\n\\n\" + \"Question : {question}\\n\"\n",
        "\n",
        "  promp_template_for_processing = f\"Answer this Question ({question}) based only on the given context.\\n\\n\" \\\n",
        "+ f\"Context: {context}\\n\\n\" + f\"Question : {question}\\n\"\n",
        "\n",
        "  prompt =PromptTemplate(template=promp_template, input_variables=[\"question\"])\n",
        "  chain = LLMChain(llm=llm, prompt=prompt, verbose=False)\n",
        "\n",
        "  result = chain.run(question=question).replace(promp_template_for_processing,\"\")\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "2BKijbPAh3X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    question = input(\"Ask me a question? (Type 'exit' to end): \")\n",
        "    if question.lower() == \"exit\":\n",
        "        break\n",
        "    result = final_result(question)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "Ju2F-o2sh5C0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}